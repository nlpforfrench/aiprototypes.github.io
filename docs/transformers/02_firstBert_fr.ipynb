{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Classification de commentaires avec Camembert sans prise de t√™te : les fondamentaux üá´üá∑\n",
    "\n",
    "[Xiaoou WANG](xiaoouwang.github.io)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Motivation\n",
    "\n",
    "Camembert a √©t√© publi√© en juin 2020. Cependant force est de constater que l'emploi de Bert en fran√ßais (Il s'agit plut√¥t de Roberta pour Camembert, voir [10 questions rapides sur Bert](01_theorie)) n'est pas encore une tendance. Nous pensons que cela est en partie d√ª au manque de tutoriels sur l'emploi des mod√®les pr√©-entra√Æn√©s.\n",
    "\n",
    "Ceci est le deuxi√®me d'une s√©rie de 10 tutoriels sur Camembert. Dans ce tuto nous allons voir notamment comment utiliser Camembert sans fine-tuning, ce dernier pr√©supposant des connaissances relativement plus pouss√©es.\n",
    "\n",
    "Nous commen√ßons par installer `transformers` et `sentencepiece`, deux packages n√©cessaires √† l'usage de Camembert. Quelques autres packages courants en machine learning ont aussi √©t√© import√©s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install sentencepiece\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Donn√©es\n",
    "\n",
    "Ensuite nous importons les donn√©es. Il s'agit d'un petit jeu de donn√©es que j'ai trouv√© [ici](https://medium.com/@vitalshchutski/french-nlp-entamez-le-camembert-avec-les-librairies-fast-bert-et-transformers-14e65f84c148). Cela vous permettra notamment de r√©pliquer ce tutoriel sur votre propre ordinateur, tant l'usage de GPU est peu n√©cessaire.\n",
    "\n",
    "La structure du dataframe est simple. Il y a une colonne commentaire avec quelques autres colonnes annotant la classe de ce commentaire. J'ai filtr√© les autres classes en ne gardant que `temps` pour utiliser une simple r√©gression logistique sur les donn√©es. 1 signifie que le commentaire est li√© √† des probl√®mes de temps d'attente et 0 non."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 322\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>temps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>accueil moyen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>.trop d'attente.insupportable trop de stress</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>la boutique</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>pas trop de monde contrairement √† d'autres bou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>1h30 d‚Äôattente pour un service irrespectueux.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review  temps\n",
       "10                                    accueil moyen...      0\n",
       "273       .trop d'attente.insupportable trop de stress      1\n",
       "84                                         la boutique      0\n",
       "81   pas trop de monde contrairement √† d'autres bou...      0\n",
       "276      1h30 d‚Äôattente pour un service irrespectueux.      1"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url=\"https://raw.githubusercontent.com/nlpinfrench/nlpinfrench.github.io/master/source/labeled_data.csv\"\n",
    "df = pd.read_csv(url,header=1,names = ['a','review','b','c','temps','e'])\n",
    "# Report the number of sentences.\n",
    "print('Number of sentences: {:,}\\n'.format(df.shape[0]))\n",
    "# remove unuseful columns\n",
    "df = df[[\"review\",\"temps\"]]\n",
    "# Display 5 random rows from the data.\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Mod√®le et Tokenizer\n",
    "\n",
    "Ensuite nous allons importer le mod√®le, le tokenizer et les weights pr√©-entra√Æn√©s. Les weights sont comme les word embeddings. Deux choses √† noter :\n",
    "\n",
    "1. Bert a son propre tokenizer avec un vocabulaire fixe. Il est donc inutile de tok√©nisez vous-m√™me.\n",
    "\n",
    "2. Ces weights sont issus du mod√®le √† l'√©tat \"brut\".\n",
    "\n",
    "En pratique vous devez fine-tuner Camembert sur des donn√©es √† vous afin d'avoir des weights propres √† chaque t√¢che et corpus.\n",
    "\n",
    "Par contre, le but de ce tuto n'est pas de fine-tuner le mod√®le mais de vous montrer que vous pouvez utiliser Bert avec les choses que vous connaissez d√©j√† en word embeddings. Il s'agit d'une s√©ance de \"d√©sintimidation\" :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load model, tokenizer and weights\n",
    "camembert, tokenizer, weights = (ppb.CamembertModel, ppb.CamembertTokenizer, 'camembert-base')\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer.from_pretrained(weights)\n",
    "model = camembert.from_pretrained(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Bert ne sait que tok√©niser des phrases de longueur maximale de 512 tokens. Ici nous allons simplement enlever les commentaires trop longs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annoying review at 314 with length 556\n",
      "Max sentence length:  556\n"
     ]
    }
   ],
   "source": [
    "# see if there are length > 512\n",
    "max_len = 0\n",
    "for i,sent in enumerate(df[\"review\"]):\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "    if len(input_ids) > 512:\n",
    "        print(\"annoying review at\", i,\"with length\",\n",
    "              len(input_ids))\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# remove > 512 sentence\n",
    "df.drop([314],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Padding\n",
    "\n",
    "Maintenant que la phrase la plus longue enlev√©e, nous faisons un padding de 472 tokens pour homog√©n√©iser la longueur de phrases. Cela rendra l'entra√Ænement plus simple. Nous indiquons aussi o√π se trouve les paddings avec `np.where` pour que Bert sache traiter les tokens de padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(321, 472)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = df['review'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "np.array(padded).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(321, 472)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Utiliser l'encodeur (encoder)\n",
    "\n",
    "Enfin nous transformer les tokens en tensor pour les passer dans le fameux transformer. Seule la derni√®re couche est conserv√©e pour faire la classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(padded)\n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Entra√Æner un mod√®le logistique\n",
    "\n",
    "Comme nous avons seulement besoin du premier token (CLS qui signifie classification) pour le mod√®le logistique, nous faisons en slice avec `[:,0,:]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "features = last_hidden_states[0][:,0,:].numpy()\n",
    "labels = df.temps\n",
    "labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Que l'entra√Ænement commence ! Nous commen√ßons par faire un split train/test avec Scikit-Learn. Ensuite nous utilisons Grid Search pour essayer de trouver le meilleur param√®tre. Finalement on entra√Æne le mod√®le."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters:  {'C': 5.263252631578947}\n",
      "best scrores:  0.8625\n"
     ]
    }
   ],
   "source": [
    "# Grid search\n",
    "parameters = {'C': np.linspace(0.0001, 100, 20)}\n",
    "grid_search = GridSearchCV(LogisticRegression(), parameters)\n",
    "grid_search.fit(train_features, train_labels)\n",
    "\n",
    "print('best parameters: ', grid_search.best_params_)\n",
    "print('best scrores: ', grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=5.263252631578947, class_weight=None, dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf = LogisticRegression(C=grid_search.best_params_['C'])\n",
    "lr_clf.fit(train_features, train_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## R√©sultats et conclusions\n",
    "\n",
    "Nous arrivons donc √† une pr√©cision de 91.36%. Si nous avions utilis√© un classifieur al√©atoire la pr√©cision aurait √©t√© autour de 60%.\n",
    "\n",
    "Voil√† ! Bravo d'avoir lu jusqu'ici. Nous esp√©rons que vous avez vu que finalement Bert n'√©tait pas si difficile √† comprendre. Il s'agit juste d'un encoder auquel on ajoute un algorithme de classification.\n",
    "\n",
    "La vraie force de Bert r√©side dans ses possibilit√©s de fine-tuning. A bient√¥t donc pour un cas pratique en classification de documents :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9135802469135802"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf.score(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy classifier score: 0.621 (+/- 0.15)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "clf = DummyClassifier()\n",
    "\n",
    "scores = cross_val_score(clf, train_features, train_labels)\n",
    "print(\"Dummy classifier score: %0.3f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## R√©f√©rences principales :\n",
    "\n",
    "https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\n",
    "\n",
    "https://camembert-model.fr/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
