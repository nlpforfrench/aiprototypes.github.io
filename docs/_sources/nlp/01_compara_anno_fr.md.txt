# Comparer Spacy, StanfordNLP et TreeTagger sur un corpus oral et un corpus de presse ğŸ‡«ğŸ‡·

[Xiaoou WANG](https://xiaoouwang.github.io), [Xingyu LIU](https://www.linkedin.com/in/xingyu-liu-aba896a1/)

## Motivation

Lâ€™annotation morphosyntaxique est une tÃ¢che consistant Ã  assigner des informations grammaticales Ã  chaque mot dâ€™un
texte. Un tagger (ou Ã©tiqueteur) est typiquement couplÃ© Ã  un outil de segmentation en phrases et en mots et/ou Ã  un
lemmatiseur.

Cette tÃ¢che est effectuÃ©e en amont des analyses textuelles plus poussÃ©es telles que lâ€™analyse sentimentale et lâ€™analyse
stylomÃ©trique et la qualitÃ© de ces derniÃ¨res dÃ©pend considÃ©rablement du choix dâ€™un bon Ã©tiqueteur morphosyntaxique.

Le but de ce rapport est dâ€™Ã©valuer la performance de 3 Ã©tiqueteurs morphosyntaxiques sur deux corpus issus du mÃªme
domaine (santÃ©) mais de registres linguistiques bien distincts : lâ€™un de nature journalistique et lâ€™autre dâ€™un niveau
plus familier.

Cette Ã©valuation Ã  deux facteurs (Ã©tiqueteur et corpus) est justifiÃ©e par les constats suivants :

1. Des difficultÃ©s ont Ã©tÃ© rapportÃ©es pour lâ€™annotation des corpus de franÃ§ais non Ã©crit (dans le sens du fond mais pas
   de la forme, un texte Ã©crit peut trÃ¨s bien retranscrire une conversation orale). Pour le franÃ§ais oral par exemple
   des tÃ©moignages ont Ã©tÃ© nombreux (voir entre autres (Benzitoun et al., 2012; Eshkol-Taravella et al., 2011)) parce
   que le langage oral possÃ¨de de nombreuses caractÃ©ristiques propres Ã  son genre (hÃ©sitations, rÃ©pÃ©titions, reprises,
   incises, amorces de mot, etcâ€¦).

2. Quant Ã  la comparaison dâ€™Ã©tiqueteurs, il est souvent question de comparer les rÃ©sultats au niveau de la prÃ©cision (
   mesures de prÃ©cision, rappel et F-mesure). Mais une vue plus globale sur les particularitÃ©s de diffÃ©rents
   Ã©tiqueteurs : temps dâ€™exÃ©cution, mÃ©thodes dâ€™entraÃ®nement, possibilitÃ©s de customisation est plus utile pour apprÃ©cier
   tant de facteurs contribuant Ã  la dÃ©cision finale dâ€™un tel ou tel Ã©tiqueteur.

## Ã‰tiqueteurs/parseurs choisis

Les 3 Ã©tiqueteurs sont Treetagger (Schmid, 1994), Spacy (Explosion, 2017) et StanfordNLP, rebaptisÃ© aujourd'hui en
Stanza (Qi et al., 2020).

## Constitution du corpus

Nous avons construit deux corpus en aspirant des textes de la rubrique santÃ©
de `le monde`(https://www.lemonde.fr/sante/) et `doctissimo`(https://forum.doctissimo.fr/sante/). La documentation de ce
processus se trouve [ici](../web/01_lemonde).

100 articles journalistiques et 100 posts de forum ont Ã©tÃ© utilisÃ©s comme corpus de base.

## Analyses textuelles exploratoires

Pour confirmer le thÃ¨me de notre corpus nous avons scrapÃ© deux autres corpus ayant comme thÃ¨me `sport` et `Ã©conomie`ã€‚

AprÃ¨s avoir normalisÃ© les textes voici les 10 mots les plus courants du corpus santÃ©.

```{figure} 01_compara_annoimages/sante.png
:align: center
Graphe des 10 mots les plus frÃ©quents du corpus de santÃ©
```

Par contraintes dâ€™espace nous nâ€™allons pas afficher le mÃªme type de graphes pour le corpus de forum. Notez simplement
que les 3 mots les plus courants sont Â« mÃ©decin Â», Â« dos Â» et Â« lombago Â», ce qui tend Ã  confirmer la validitÃ© de ce
corpus aussi.

Comme cette mÃ©thode ne permet pas d'avoir une vue plus globale de la spÃ©cificitÃ© du corpus, nous avons construit des
graphes de corrÃ©lation basÃ©s sur l'occurrence de mots dans les textes de chaque thÃ¨me. Le graphe est assez facile Ã 
interprÃ©ter : 1) plus un token sâ€™approche de la ligne, plus la frÃ©quence de ce premier est proche dans deux tableaux 2)
plus il y a de mots Ã  proximitÃ© de la ligne, plus les deux tableaux contiennent les mÃªmes mots Ã  frÃ©quence semblable.

Le nombre considÃ©rable de mots de frÃ©quence similaire entre le corpus santÃ© sÃ©parÃ© en deux montre encore une fois la
validitÃ© de lâ€™hypothÃ¨se selon laquelle les articles de diffÃ©rents thÃ¨mes sont caractÃ©risÃ©s par les mots non-vides les
plus frÃ©quents.

```{figure} 01_compara_annoimages/correlation.png
:align: center
Graphe de corrÃ©lation pour 3 corpus de diffÃ©rents genres
```

## Ã‰chantillonnage des corpus et standardisation des Ã©tiquettes d'annotation

### StratÃ©gie dâ€™Ã©chantillonnage des deux corpus

Vu la quantitÃ© de textes et le manque de personnel il nous a paru impossible dâ€™annoter tous les deux corpus. Il nâ€™est
pas pour autant prÃ©fÃ©rable dâ€™annoter uniquement quelques articles ou quelques posts pour faire lâ€™Ã©valuation car
lâ€™intÃ©rÃªt de lâ€™aspiration se trouvera fortement diminuÃ©.

Nous avons dÃ©cidÃ© dâ€™extraire un Ã©chantillon de chacun des deux corpus avec une mÃ©thode qui permettrait Ã  cet Ã©chantillon
de reprÃ©senter plus ou moins lâ€™ensemble du corpus.

Par souci de concision, nous ne prenons uniquement le corpus de presse comme exemple pour dÃ©montrer la mÃ©thodologie.
Lâ€™Ã©chantillonnage a Ã©tÃ© effectuÃ© avec le mÃªme principe pour le corpus de forum.

Nous avons dâ€™abord calculÃ© la distribution des longueurs de mots de lâ€™ensemble des phrases du corpus de presse. Les 4
quartiles de la distribution sont 16, 25, 36 et 115, Ã  savoir que Â¼ de phrases ont une longueur <= 16 et ainsi de suite.
La figure suivante illustre cette distribution :

```{figure} 01_compara_annoimages/distribution.png
:align: center
Distribution de longueurs de phrases du corpus de presse
```

Pour que ces quatre catÃ©gories de phrases soient Ã©galement reprÃ©sentÃ©es, nous avons gÃ©nÃ©rÃ© 60 nombres alÃ©atoires par
catÃ©gorie pour extraire alÃ©atoirement 60 phrases par catÃ©gorie.

Ã€ la fin de cette phase nous avons pu recenser 6798 mots pour le corpus de presse et 6094 pour le corpus de forum. Ces
deux Ã©chantillons constituent une bonne base pour garantir la comparabilitÃ© concernant lâ€™annotation de ces deux types de
corpus.

### Standardisation des Ã©tiquettes

Si les deux taggers Spacy et StanfordNLP ont recours aux Ã©tiquettes de Universal Dependencies (
UD, [Nivre et al., 2016](https://universaldependencies.org/u/pos/all.html#al-u-pos/NUM)), Treetagger utilise des
Ã©tiquettes basÃ©es sur le jeu dâ€™Ã©tiquettes de Penn
Treebank ([Marcus et al., 1993](https://www.cis.unimuenchen.de/~schmid/tools/TreeTagger/data/french-tagset.html)). Ces
deux systÃ¨mes ne sont pas entiÃ¨rement convertibles. Par exemple lâ€™Ã©tiquette Â« aux Â» nâ€™existe pas dans Treetagger
franÃ§ais et de lâ€™autre cÃ´tÃ©, lâ€™Ã©tiquette Â« SYM Â» en UD nâ€™a pas dâ€™Ã©quivalent dans Treetagger.

Le cas le plus dÃ©licat est lâ€™Ã©tiquette de Â« SCONJ Â» en UD qui recense des conjonctions de subordination telles que Â« que
Â», Â« si Â» et Â« quand Â», etc. alors que ces mots sont tous Ã©tiquetÃ©s en Â« PRO:REL Â» (pronoms relatifs) dans Treetagger.

Treetagger aussi propose une Ã©tiquette Â« ABR Â» (abrÃ©viation) qui est difficilement dÃ©finissable, nous lâ€™avons assimilÃ© Ã 
la catÃ©gorie Â« PROPN Â» (nom propre) en UD.

Le tableau suivant dÃ©taille notre choix pour convertir au maximum les Ã©tiquettes de Treetagger en UD.

```{figure} 01_compara_annoimages/e606f6f0.png
:align: center
```

```{figure} 01_compara_annoimages/8dba10bb.png
:align: center
Conversion des tags de TreeTagger en tags d'UD  
```

Une derniÃ¨re question porte sur la lemmatisation. Treetagger parfois attribue une sous-Ã©tiquette @card@ Ã  des chiffres.
Nous avons transformÃ© systÃ©matiquement ce lemme en token original pour garantir la fiabilitÃ© des mesures ultÃ©rieures.

### Quelques problÃ¨mes mineurs

Il nous a paru utile de prÃ©ciser ici quelques problÃ¨mes rencontrÃ©s durant la standardisation dâ€™Ã©tiquettes pour permettre
une meilleure reproductibilitÃ© de codes :

1. Les trois points Â« â€¦ Â» sâ€™Ã©tiquettent tantÃ´t comme trois symboles tantÃ´t comme un seul symbole. Nous avons standardisÃ©
   la transcription en trois symboles pour Ã  la fois rendre lâ€™annotation plus comparable et Ã©viter des problÃ¨mes
   dâ€™encodage.

2. Spacy nâ€™arrive pas Ã  tokÃ©niser â€˜Â«â€™ et â€˜Â»â€™. En plus de ne pas reconnaÃ®tre cette ponctuation, il insÃ¨re
   systÃ©matiquement une ligne vide aprÃ¨s ces deux tokens. Cela rend les mesures de prÃ©cision trÃ¨s difficiles et nous
   avons dÃ» remplacer tous les guillemets franÃ§ais par les guillemets anglais dans les deux Ã©chantillons de corpus avant
   de les faire annoter automatiquement.

3. Les espaces insÃ©cables peuvent aussi causer des problÃ¨mes dâ€™annotation. Nous les avons remplacÃ©s tous par des espaces
   normaux.

## Ã‰valuation des taggers

### PrÃ©sentation gÃ©nÃ©rale des taggers

Les taggers morphosyntaxiques sont nombreux sur le marchÃ©, ci-dessous une liste de quelques taggers pour le franÃ§ais
utilisÃ©s dans la littÃ©rature :

```{figure} 01_compara_annoimages/266bcab0.png
:align: center
Liste non exhaustive de diffÃ©rents taggers disponibles pour le franÃ§ais (https://github.com/clement-plancq/outils-corpus)
```

Le principe des taggers peut Ãªtre fort simple. Un algorithme simpliste qui catÃ©gorise les mots connus dans le tag le
plus courant et les mots inconnus dans les noms propres peut atteindre une prÃ©cision globale de 90% sur des donnÃ©es en
anglais (Charniak, 1997).

Cependant des difficultÃ©s considÃ©rables existent pour atteindre une prÃ©cision plus Ã©levÃ©e. Entre les principales
difficultÃ©s nous pouvons citer le fait curieux que la plupart des types formant le vocabulaire sont non ambigus qui ne
peuvent porter quâ€™une catÃ©gorie morphosyntaxique. Or, ces derniers sont les plus frÃ©quents dans les textes.

```{figure} 01_compara_annoimages/e90103e4.png 
:align: center
RÃ©partition des mots ambigus et non-ambigus dans deux corpus dâ€™anglais (Jurafsky, 2000)
```

DiffÃ©rents types dâ€™Ã©tiqueteurs existent.

Des Ã©tiqueteurs Ã  bases de rÃ¨gles qui utilisent lexique (forme flÃ©chie, POS, morpho, lemme) et de grammaires locales (
ex : Unitex). Ce type dâ€™Ã©tiqueteurs ont notamment lâ€™avantage dâ€™avoir des rÃ¨gles explicites et modifiables aisÃ©ment et
dâ€™Ãªtre facilement implÃ©mentables (automates finis). Mais lâ€™Ã©criture manuelle des rÃ¨gles est difficile et coÃ»teux et
lâ€™adaptation Ã  de nouvelles donnÃ©es difficile, ce qui fait quâ€™ils ne sont plus trÃ¨s utilisÃ©s aujourd'hui.

Une autre famille dâ€™Ã©tiqueteurs est basÃ©e sur des mÃ©thodes probabilistes, ce qui les rend trÃ¨s performants. Le comble,
câ€™est que ce genre de mÃ©thodes nÃ©cessite moins dâ€™expertise par rapport aux Ã©tiqueteurs Ã  bases de rÃ¨gles. Les chaÃ®nes de
Markov sont au cÅ“ur de la plupart des algorithmes utilisÃ©s et de nombreuses variantes sont possibles (entropie maximale,
SVM, CRF, etc.)

Cependant les taggers basÃ©s sur lâ€™apprentissage statistique nÃ©cessitent un corpus de rÃ©fÃ©rence annotÃ©. Ce type de
protocoles nÃ©cessitent un grand investissement de temps. De plus, il nâ€™est pas toujours aisÃ© dâ€™analyser les erreurs.

### Spacy, TreeTagger et StanfordNLP

Les taggers morphosyntaxiques utilisÃ©s ici sont Spacy, TreeTagger et StanfordNLP (dÃ©sormais appelÃ© Stanza). Il est Ã 
noter que le nom du dernier tagger peut prÃªter Ã  confusion car il existe aussi un framework appelÃ© Stanford CoreNLP (
Manning et al., 2014) qui est surtout connu pour sa performance sur des donnÃ©es en anglais et ses fonctions avancÃ©es
telles que rÃ©solution de corÃ©fÃ©rence. Nous verrons dans la suite de cette section que ce framework nâ€™est pas adaptÃ©
notre projet et que le StanfordNLP utilisÃ© dans ce rapport est bien le StanfordNLP en Python appelÃ© aujourd'hui Stanza.

Tous ces trois taggers sont des taggers basÃ©s sur des mÃ©thodes probabilistes. Le cÅ“ur des algorithmes de ces taggers
repose sur des ngram utilisÃ©s pour modÃ©liser la probabilitÃ© dâ€™une sÃ©quence de mots tagguÃ©s.

* TreeTagger repose Ã©galement sur des ngram mais il utilise des arbres de dÃ©cision binaires pour estimer les
  probabilitÃ©s de transition entre les mots de la sÃ©quence (Schmid, 1994). Câ€™est un modÃ¨le Ã©crit en C et trÃ¨s utilisÃ©
  pour le franÃ§ais car il est gratuit (mais propriÃ©taire), rapide et propose de nombreux modÃ¨les. Cependant, on ne sait
  presque rien sur les donnÃ©es utilisÃ©es pour entraÃ®ner le modÃ¨le franÃ§ais. Les Ã©tiquettes de TreeTagger sont basÃ©es sur
  le tagset de Penn Treebank. Pour le modÃ¨le franÃ§ais, les fonctions tokÃ©nisation, lemmatisation, annotation
  morphosyntaxique et chunking sont proposÃ©es.

* Spacy est une librairie relativement jeune (2015). Câ€™est une librairie polyvalente de Python qui dans le cas du
  franÃ§ais est Ã©quipÃ©e de tokÃ©niseur, POS tagger, tagger de dÃ©pendance syntaxique, chunkeur, identifieur de NER etcâ€¦
  Spacy est entraÃ®nÃ© sur UD French-Sequoia et WikiNER, ce qui fait quâ€™il serait moins performant sur les textes dâ€™autres
  genres comme les posts de forum. Il est Ã  noter que deux modÃ¨les existent pour le franÃ§ais, le modÃ¨le utilisÃ© dans
  notre projet est le plus grand fr_core_news_md (84 Mb). Lâ€™un de ses grands atouts est sa Â« industry level speed Â». â€¢
  Stanford CoreNLP est un framework Ã©crit en Java applicable Ã  6 langues. Câ€™est un framework puissant qui propose des
  fonctions poussÃ©es comme lâ€™analyse sentimentale. Cependant le modÃ¨le actuel pour le franÃ§ais nâ€™a pas de lemmatisateur.

* Stanford CoreNLP est un framework Ã©crit en Java applicable Ã  6 langues. Câ€™est un framework puissant qui propose des
  fonctions poussÃ©es comme lâ€™analyse sentimentale. Cependant le modÃ¨le actuel pour le franÃ§ais nâ€™a pas de lemmatisateur.

```{figure} 01_compara_annoimages/597aa052.png
:align: center
FonctionnalitÃ©s disponibles de Stanford CoreNLP pour 6 langues
```

Enfin StanfordNLP est un framework de Python basÃ© sur PyTorch et applicable Ã  66 langues Ã  lâ€™heure actuelle. Câ€™est un
framework puissant et Ã©galement un grand consommateur de capacitÃ©s de calcul car il est basÃ© sur des rÃ©seaux de
neurones.

```{figure} 01_compara_annoimages/f9aca8e1.png
:align: center
CaractÃ©ristiques techniques de StanfordNLP par rapport Ã  dâ€™autres frameworks en TAL (Qi et al., 2020)
```

### Ã‰valuation sur le plan de la vitesse de traitement

Pour cette tÃ¢che nous avons construit un gros corpus de 568 000 mots en dupliquant simplement un corpus de base. Les
commandes utilisÃ©es pour Ã©valuer les 4 taggers sont :

* Stanford CoreNLP

```bash
command time java-Xmx6g -cp "*" edu.stanford.nlp.pipeline.StanfordCoreNLP -props french.properties
```

* TreeTagger

```bash
command time tree-tagger-french testAnnotator.txt
```

* Spacy

```python
t0 = time()
nlp = spacy.load("fr_core_news_md", disable=["ner", "parser"])
nlp.max_length = 343000  # as long as > 1000000 and you don't run out of RAM
doc = nlp(text)
t1 = time()
```

* StanfordNLP (Stanza)

```python
t0 = time()
nlp = stanza.Pipeline(lang='fr', processors='tokenize,mwt,pos,lemma,depparse')
doc = nlp(text)
t1 = time()
```

Nous avons montrÃ© le code car la comparaison de vitesse est une tÃ¢che bien dÃ©licate. Sans spÃ©cifier la configuration on
peut parfois Ãªtre littÃ©ralement ahuri par des Ã©valuations fournies sur Internet. StanfordNLP fournit par exemple un
record de 4.51 millions de mots par seconde sur [leur page](https://nlp.stanford.edu/software/tokenizer.shtml) alors
quâ€™il sâ€™agit simplement dâ€™une tÃ¢che de tokÃ©nisation effectuÃ©e en plus par un tokÃ©niseur par rÃ¨gles. Spacy paraÃ®trait
bien plus lent par rapport Ã  cet exploit car il nâ€™affiche que 13 963 mots par seconde
sur [sa page officielle](https://spacy.io/usage/facts-figures). Or il nâ€™en est rien car Spacy tient compte aussi du
processus dâ€™annotation de dÃ©pendance syntaxique. Câ€™est pour dire combien il est important de prÃ©ciser les paramÃ¨tres
lorsqu'on parle de vitesse de traitement.

Dans notre cas, tous les 4 taggers (sauf Stanford CoreNLP) ont effectuÃ© une tÃ¢che de tokÃ©nisation + lemmatisation +
annotation morphosyntaxique. Le tableau suivant rÃ©sume les rÃ©sultats du test.

|       Framework      | Temps de traitement pour 568000 mots | Vitesses de traitement (mots par seconde) |
|:--------------------:|--------------------------------------|-------------------------------------------|
|      TreeTagger      | 71.97 secondes                       | 8000 m/s                                  |
|    StanfordCoreNLP   | 609.5 secondes                       | 568 m/s                                   |
|         Spacy        | 60 secondes                          | 9467 m/s                                  |
| StanfordNLP (Stanza) | 20 minutes                           | 473 m/s                                   |

Vitesse d'annotation de 4 taggers sur un corpus de 568000 mots

Pour mieux apprÃ©cier la validitÃ© de notre test, voici les caractÃ©ristiques techniques de notre ordinateur :

```{figure} 01_compara_annoimages/d18df612.png
:align: center
Configuration de lâ€™ordinateur utilisÃ© pour Ã©valuer la vitesse dâ€™annotation de 4 taggers
```

Nous voyons bien que Stanford NLP est le plus lent et Spacy le plus rapide. Cependant quelques remarques prÃ©liminaires
mÃ©ritent dâ€™Ãªtre avancÃ©es ici. Tout dâ€™abord Spacy est un modÃ¨le mettant lâ€™accent sur la vitesse, comme fait remarquer
avec pertinence lâ€™auteur de Stanford NLP dans son article (Qi et al., 2020). Nous verrons par la suite quâ€™au niveau de
la prÃ©cision Câ€™est Stanford NLP qui lâ€™emporte. Ensuite Stanford NLP permet une accÃ©lÃ©ration considÃ©rable si une carte
GPU est disponible. Nous nâ€™avons pas pu faire un essai faute dâ€™Ã©quipement. Cependant des entreprises peuvent trÃ¨s bien
avoir les moyens de sâ€™en fournir, ce qui rÃ©duire lâ€™Ã©cart de vitesse entre les deux frameworks.

TreeTagger sâ€™est classÃ© le deuxiÃ¨me. Cependant la vitesse de TreeTagger, comme nous le verrons plus tard, est obtenue au
prix dâ€™une perte considÃ©rable de prÃ©cision.

Enfin un dernier mot sur Stanford CoreNLP. La vitesse de traitement est lente. Elle sera dâ€™autant plus lente si lâ€™on
considÃ¨re que la tÃ¢che de lemmatisation nâ€™a pas Ã©tÃ© effectuÃ©e car indisponible pour le franÃ§ais (voir plus haut). De
plus il faut rappeler quâ€™ici nous avons utilisÃ© le prÃ©fixe java -Xmx6g, Ã  savoir que nous avons allouÃ© 6g de mÃ©moires au
programme pour la tÃ¢che en question. En effet nous avons fait plusieurs tests et il sâ€™est avÃ©rÃ© que 6g de mÃ©moires est
un minimum pour que le programme puisse aboutir. Compte tenu de ces facteurs nous avons dÃ©cidÃ© dâ€™examiner uniquement
StanfordNLP pour la suite de lâ€™Ã©valuation.

### Ã‰valuation sur le plan de la prÃ©cision

Dans les sous-sections qui suivent nous prÃ©sentons notamment les mesures de prÃ©cision, de rappel et le F-mesure tout en
sachant que dans des tÃ¢ches dâ€™annotation on met en avant la prÃ©cision.

Les deux Ã©chantillons de corpus ont Ã©tÃ© annotÃ©s manuellement par nos soins, servant de Â« gold standard Â».

Voici les premiÃ¨res lignes de notre gold standard du corpus de forum, notons lâ€™annotation de Â« a Â» en adposition car il
sâ€™agit ici dâ€™une faute dâ€™orthographe typique de ce genre de corpus :

Nous commenÃ§ons par les tests sur le corpus de presse.

| Metric   |   col1 |   col2 |    f1 |
|:---------|-------:|-------:|------:|
| Tokens   |  99.87 |  99.82 | 99.84 |
| UPOS     |  97.15 |  97.1  | 97.12 |
| Lemmas   |  95.21 |  95.16 | 95.18 |

Tableau des mesures de prÃ©cision de StanfordNLP sur le corpus de presse

Nous voyons que les rÃ©sultats de StanfordNLP ont Ã©tÃ© plutÃ´t satisfaisant sur le corpus de presse. Cela est plutÃ´t
prÃ©dictible car il sâ€™agit dâ€™un corpus de franÃ§ais Ã©crit relativement formel. Nous menons une discussion en dÃ©tail dans
une sous-section dÃ©diÃ©e spÃ©cialement Ã  lâ€™analyse des erreurs et nous nous contentons de donner des rÃ©sultats en chiffres
dans cette sous-section.

Les rÃ©sultats de Spacy sont moins bons que StanfordNLP mais assez satisfaisants :

| Metric   |   precision |   recall |    f1 |
|:---------|------------:|---------:|------:|
| Tokens   |       97.69 |    99.12 | 98.4  |
| UPOS     |       90.53 |    91.85 | 91.19 |
| Lemmas   |       87.77 |    89.05 | 88.41 |

Tableau des mesures de prÃ©cision de Spacy sur le corpus de presse

Enfin, les rÃ©sultats de TreeTagger sont les moins bons :

| Metric   |   precision |   recall |    f1 |
|:---------|------------:|---------:|------:|
| Tokens   |       93.71 |    89    | 91.29 |
| UPOS     |       83.72 |    79.52 | 81.57 |
| Lemmas   |       90.25 |    85.73 | 87.93 |

Tableau des mesures de prÃ©cisions de TreeTagger sur le corpus de presse

Voici les rÃ©sultats des tests sur le corpus de forum.

Nous commenÃ§ons toujours par le tagger de StanfordNLP :

| Metric   |   precision |   recall |    f1 |
|:---------|------------:|---------:|------:|
| Tokens   |       92.5  |    91.34 | 91.92 |
| UPOS     |       90.75 |    91.59 | 91.17 |
| Lemmas   |       93.68 |    93.52 | 93.6  |

Tableau des mesures de prÃ©cision de StanfordNLP sur le corpus de forum

Le tagger de Spacy :

| Metric   |   precision |   recall |    f1 |
|:---------|------------:|---------:|------:|
| Tokens   |       88.92 |    86.79 | 87.84 |
| UPOS     |       87.29 |    88.17 | 87.73 |
| Lemmas   |       86.56 |    87.43 | 86.99 |

Tableau des mesures de prÃ©cison de Spacy sur le corpus de forum

Et enfin le tagger de TreeTagger, toujours le moins prÃ©cis sur tous les plans :

| Metric   |   precision |   recall |    f1 |
|:---------|------------:|---------:|------:|
| Tokens   |       85.69 |    84.79 | 85.24 |
| UPOS     |       79.66 |    78.91 | 79.28 |
| Lemmas   |       79.83 |    79.08 | 79.45 |

Tableau des mesures de prÃ©cision de TreeTagger sur le corpus de forum

## Discussions gÃ©nÃ©rales

Nous commenÃ§ons par quelques remarques gÃ©nÃ©rales valables pour les deux types de corpus :

Sur les plans de la mesure de prÃ©cision, de la mesure de rappel et de F-mesure :

* La performance des 3 taggers sur le corpus de presse a Ã©tÃ© systÃ©matiquement meilleur que sur le corpus de forum.
  Lâ€™Ã©cart en moyenne est 7.2% pour la mesure de prÃ©cision, 5.6% pour la mesure de rappel et 6.3% pour F1-mesure.

* La performance de StanfordNLP a Ã©tÃ© systÃ©matiquement meilleure que les deux autres taggers sur les deux types de
  corpus, que ce soit pour la tokÃ©nisation, la lemmatisation et le POS tagging. Spacy se classe le deuxiÃ¨me et
  TreeTagger le dernier. Lâ€™Ã©cart moyen pour les trois tÃ¢ches est 3.4% (versus Spacy) et 3.6% (versus TreeTagger) pour la
  mesure de prÃ©cision sur le corpus de presse et 5.4% et 10.3% respectivement sur le corpus de forum.

Pour approfondir lâ€™analyse, nous donnons ici une typologie des erreurs et le comportement des 3 taggers face Ã 
diffÃ©rentes erreurs :

* Les 3 taggers ont Ã©tÃ© relativement prÃ©cis sur le corpus de presse. Les principales sources dâ€™erreurs viennent de
  lâ€™annotation des noms propres. Lorsque ces derniers ne commencent pas par une lettre capitale, tous les 3 taggers ont
  pÃªchÃ©, et ce mÃªme pour des noms propres trÃ¨s courants tels que France.

* Pour le corpus de forum, les principales erreurs proviennent des Â« fautes dâ€™orthographes Â». Cette catÃ©gorie de formes
  comprend : o des mots rÃ©ellement mal Ã©pelÃ©s (aprÃ©s, Ã§Ã ), mais des fautes comme Â« je regardes Â», Â« je vais lacher (qui
  aurait dÃ» Ãªtre lÃ¢cher) Â» sont bien tolÃ©rÃ©es au niveau du POS Tagging car il sâ€™agit toujours dâ€™un verbe. Mais la
  lemmatisation reste incorrecte. TreeTagger est le tagger supportant le moins dâ€™erreurs dâ€™orthographes car elles ont
  Ã©tÃ© systÃ©matiquement classÃ©es comme X (unknown), entraÃ®nant une chute de performance.

StanfordNLP gÃ¨re le mieux au niveau les fautes. Il arrive mÃªme Ã  restaurer la bonne forme pour des erreurs comme Â« ca Â»
pour Â« Ã§a Â». Il arrive aussi Ã  classer Â« trÃ©s Â» comme adverbe en se basant sur la distribution des ngrammes.

Spacy se situe entre les deux. Mais il est dÃ©cidÃ©ment meilleur que TreeTagger.

* Les points oÃ¹ aucun tagger se dÃ©marque des autres

Des mots distincts soudÃ©s (ilya ; moi,cela ; Bonjourje ; 20euros). Aucun tagger nâ€™est capable de restituer les formes et
faire une tokÃ©nisation correcte.

Des mots Â« propres Â» au langage du forum et dans une moindre mesure au langage familier (@+, + ou -, mm pour mÃªme,
ptite, merciiiiiiiiiiiiii, rha alala). Aucun tagger nâ€™est capable de faire une annotation correcte.

Des Interjections propres au forum ( ;-) ). Aucun tagger nâ€™est capable de traiter ce genre de tokens.

* TreeTagger ne gÃ¨re pas les symboles. Â« g Â», Â« â‚¬ Â» sont systÃ©matiquement annotÃ©s comme X. Stanford NLP gÃ¨re le mieux.

* TreeTagger ne gÃ¨re pas non plus les formes raccourcis (kinÃ©). Encore une fois StanfordNLP est le meilleur concernant
  lâ€™annotation de ce genre de tokens.

Enfin il nous a paru bon de prÃ©ciser la belle performance de StanfordNLP et de Spacy sur les noms propres. La plupart
des noms propres ont Ã©tÃ© bien tagguÃ©s quand ils commencent par une lettre en majuscule, que cela soit des noms de
mÃ©dicament (Levothyrox, KÃ©toprofÃ¨ne) ; des noms de virus (VHC, Zika, Ebola) ; ou encore des noms de maladie (SGB, SCT).

StanfordNLP Â« reconnaÃ®t Â» mÃªme des parties de corps (prothÃ¨se discale en L4-L5) ou des types de voitures Â« 4x4 Â».
Cependant la prudence est de mise car il peut simplement sâ€™agit dâ€™une diffÃ©rence de stratÃ©gie concernant les rÃ¨gles de
catÃ©gorisation de POS Tag et non pas dâ€™une diffÃ©rence modÃ¨le dâ€™apprentissage statistique.

Par contre sur ce point la performance de TreeTagger a Ã©tÃ© mÃ©diocre. DÃ¨s quâ€™un nom non courant commence une lettre
majuscule il est classÃ© comme X.

Enfin, nous proposons deux tableaux rÃ©capitulant les caractÃ©ristiques principales de ces 3 taggers.

|    Tagger   | Vitesse d'annotation |
|:-----------:|----------------------|
|  TreeTagger | TrÃ¨s rapide          |
|    Spacy    | Rapide               |
| StanfordNLP | Moyen                |

Tableau rÃ©capitulant la vitesse d'annotation des Taggers

|    Tagger   | ProblÃ¨mes                                                                         | Performance                                                    |
|:-----------:|-----------------------------------------------------------------------------------|----------------------------------------------------------------|
|  TreeTagger | Fautes d'orthographe<br><br>Mots liÃ©s<br><br>Noms propres<br><br>Langage du forum | Mauvais<br><br>Mauvais<br><br>Mauvais<br><br>Mauvais           |
|    Spacy    | Fautes d'orthographe<br><br>Mots liÃ©s<br><br>Noms propres<br><br>Langage du forum | Moyen<br><br>Mauvais<br><br>Bon<br><br>Mauvais                 |
| StanfordNLP | Fautes d'orthographe<br><br>Mots liÃ©s<br><br>Noms propres<br><br>Langage du forum | Bonne tolÃ©rance<br><br>Mauvais<br><br>Excellent<br><br>Mauvais |

Tableau rÃ©capitulant les diffÃ©rences de performance des Taggers face aux problÃ¨mes du corpus de forum

Pour conclure ce rapport, nous voudrions simuler un scÃ©nario oÃ¹ nous agissons en tant que conseiller informatique en
TAL :

1. Si le texte Ã  annoter est du franÃ§ais formel, utilisez plutÃ´t TreeTagger car il est ultrarapide. Cependant il peut
   Ãªtre nÃ©cessaire dâ€™ajouter quelques rÃ¨gles sur la catÃ©gorisation des noms non communs commenÃ§ant par une lettre
   majuscule.

2. Si le texte Ã  annoter est du franÃ§ais familier, utilisez plutÃ´t Spacy car il est relativement rapide et gÃ¨re
   relativement bien un franÃ§ais non standard relativement bien transcrit.

3. Si le texte est parsemÃ© abondamment de fautes dâ€™orthographes, optez pour StanfordNLP, câ€™est le framework qui supporte
   mieux ce type dâ€™erreurs. Cela est vrai surtout lorsque votre backend est Ã©quipÃ© de bons matÃ©riels (GPU de grandes
   capacitÃ©s de calcul)

4. Si le texte contient beaucoup dâ€™Ã©lÃ©ments propres Ã  un niveau de langage (comme câ€™est le cas de notre Ã©tude sur le
   franÃ§ais du forum). EntraÃ®ner vos propres modÃ¨les car peu de modÃ¨les sur le marchÃ© est prÃªt Ã  utiliser face Ã  ce
   genre de tÃ¢ches.

Quelques manques de ce rapport :

1. Faute de temps nous nâ€™avons pu transcrire quâ€™une partie de notre corpus aspirÃ©. Bien que nous ayons procÃ©dÃ© Ã  un
   Ã©chantillonnage basÃ© sur les caractÃ©ristiques distributionnelles des corpus, la taille du corpus de rÃ©fÃ©rence nâ€™est
   pas encore satisfaisante.

2. Lâ€™analyse des erreurs reste qualitative. Il est de plus prÃ©fÃ©rable de pouvoir classer les erreurs par POS Tag.

3. La critique est aisÃ©e, lâ€™art est difficile. Il serait intÃ©ressant dâ€™essayer dâ€™entraÃ®ner de nouveaux modÃ¨les pour
   pallier manque dâ€™outils adÃ©quats en vue de la transcription des corpus de forum. Comme nous avons vu dans le rapport,
   ce type de corpus recÃ¨le des particularitÃ©s comparables ni au franÃ§ais formel, ni au franÃ§ais oral.

## Notes supplÃ©mentaires

Les difficultÃ©s des parseurs actuels proviennent principalement du fait que la plupart des Ã©tiqueteurs ont Ã©tÃ© entraÃ®nÃ©s
sur des donnÃ©es utilisant un franÃ§ais relativement soutenu. A titre dâ€™exemples, French Treebank (AbeillÃ© et al., 2003) a
Ã©tÃ© construit sur un corpus tirÃ© de Â« le monde Â». Sequoia Treebank quant Ã  lui a Ã©tÃ© construit sur un mÃ©lange de textes
journalistiques et de textes de Wikipedia (Candito & Seddah, 2012).

## RÃ©fÃ©rences

AbeillÃ©, A., ClÃ©ment, L., & Toussenel, F. (2003). Building a treebank for French. In Treebanks (pp. 165â€“187). Springer.

Benzitoun, C., Fort, K., & Sagot, B. (2012). TCOF-POS: Un corpus libre de franÃ§ais parlÃ© annotÃ© en morphosyntaxe.

Candito, M., & Seddah, D. (2012). Le corpus Sequoia: Annotation syntaxique et exploitation pour lâ€™adaptation dâ€™analyseur
par pont lexical.

Charniak, E. (1997). Statistical techniques for natural language parsing. AI Magazine, 18(4), 33â€“33.

Eshkol-Taravella, I., Baude, O., Maurel, D., Hriba, L., Dugua, C., & Tellier, I. (2011).

Un grand corpus oral Â«disponibleÂ»: Le corpus dâ€™OrlÃ©ans 1 1968-2012.

Explosion, A. I. (2017). SpaCy-Industrial-strength Natural Language Processing in Python. URL: Https://Spacy. Io.

Jurafsky, D. (2000). Speech & language processing. Pearson Education India.

Kahane, S., Deulofeu, H.-J., Gerdes, K., Valli, A., & Nasr, A. (2018). Guide dâ€™annotation syntaxique OrfÃ©o (version
Platinum).

Lebart, L., & Salem, A. (1988). Analyse statistique des donnÃ©es textuelles: Questions ouvertes et lexicomÃ©trie. Dunod
Paris. Manning, C. D., Surdeanu, M., Bauer, J., Finkel, J. R., Bethard, S., & McClosky, D. (2014). The Stanford CoreNLP
natural language processing toolkit. Proceedings of 52nd Annual Meeting of the Association for Computational
Linguistics: System Demonstrations, 55â€“60.

Marcus, M., Santorini, B., & Marcinkiewicz, M. A. (1993). Building a large annotated corpus of English: The Penn
Treebank.

Mitchell, R. (2018). Web scraping with Python: Collecting more data from the modern web. Oâ€™Reilly Media, Inc.

Nivre, J., De Marneffe, M.-C., Ginter, F., Goldberg, Y., Hajic, J., Manning, C. D., McDonald, R., Petrov, S., Pyysalo,
S., & Silveira, N. (2016). Universal dependencies v1: A multilingual treebank collection. Proceedings of the Tenth
International Conference on Language Resources and Evaluation (LRECâ€™16), 1659â€“1666.

Qi, P., Zhang, Y., Zhang, Y., Bolton, J., & Manning, C. D. (2020). Stanza: A Python Natural Language Processing Toolkit
for Many Human Languages. ArXiv Preprint ArXiv:2003.07082.

Schmid, H. (1994). TreeTagger-a language independent part-of-speech tagger.