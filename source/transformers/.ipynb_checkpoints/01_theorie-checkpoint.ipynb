{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 10 questions rapides sur Bert\n",
    "\n",
    "*By [Xiaooou Wang](https://xiaoouwang.github.io/), Master’s student (currently in Paris) in NLP looking for a phd position/contrat cifre. [Blog](http://xiaoouwang.github.io)/[Linkedin](https://www.linkedin.com/in/xiaoou-wang)/[Email](mailto:xiaoouwangfrance@gmail.com)*\n",
    "\n",
    "## 1 Qu'est-ce RoBERTa, XLNet, and ALBERT ?\n",
    "\n",
    "Des modèles dérivés de Bert qui sont parvenus à de meilleures performances sur des benchmarks que Bert.\n",
    "\n",
    "## 2 Ai-je besoin de comprendre la récurrence (des modèles CNN ou RNN) pour saisir les subtilités de Bert ?\n",
    "\n",
    "Non, Bert est basé sur l'architecture transformer. En plus il n'y a que la partie encoder qui est concernée.\n",
    "\n",
    "## 3 C'est quoi du coup la différence entre RNN et Transformer ?\n",
    "\n",
    "Les réseaux de neurones récurrents ne \"regardent\" qu'un seul mot à la fois mais utilisent des hidden states pour stocker, filtrer et intégrer les informations du passé. Cela rend le modèle très compliqué rien qu'au niveau de la backpropagation (rétropropagation du gradient).\n",
    "\n",
    "Attention, les RNNS constituent un champ de recherches à part entière. Cela mérite une autre série de tutos.\n",
    "\n",
    "## 4 Lien entre Pre-Training, Fine-Tuning et apprentissage de transfert ?\n",
    "\n",
    "Bert est pré-entraîné sur un énorme corpus avec deux tâches. Vu la spécificité de ces deux tâches il est supposé (attention c'est juste une hypothèse) que le modèle acquerra une compréhension profondément contextualisée des mots (concrètement ce sont des embeddings). Mais ce modèle out-of-box n'est pas apte à des tâches en aval (downstream tasks) qui demandent un réapprentissage sur des corpus spéficiques (fine-tuning). L'étude de ceux deux processus relève de l'apprentissage de transfert.\n",
    "\n",
    "## 5 Quels intérêts ?\n",
    "\n",
    "Entraîner un modèle pour une tâche spécifique demande souvent un énorme corpus d'un domaine particulier qui n'est pas forcément disponible. En plus c'est très couteux en termes de temps et de calcul. Le processus de fine-tuning, en plus d'économiser du temps, arrive à de meilleures performances qu'un apprentissage from scratch fait maison.\n",
    "\n",
    "Attention ! Cela ne signifie pas que le fine-tuning prend quelques secondes, il est juste rapide par rapport au pré-entrainement de Bert et aux modèles faits maison. Si l'on remonte un peu dans le temps, les bénéfices de cette approche ont été d'abord constatés en reconnaissance d'images (branche de computer vision `la traduction en français, vision par ordinateur, est maladroite à notre avis.`]). Théoriquement cela a peu de choses de nouveau.\n",
    "\n",
    "## 6 Sur quelles tâches est entrainé Bert ?\n",
    "\n",
    "Tout comme pour word embedding de Mikolov, ce sont des fake tasks. La première consiste à deviner un mot masqué dans une phrase et la deuxième à déterminer si la deuxième d'une paire de phrases constitue la suite de la première.\n",
    "\n",
    "## 7 Quels avantages des embeddings Bert ?\n",
    "\n",
    "Comme le mot est masqué à l'intérieur de la phrase, Bert est censé acquérir une représentation contextualisée utilisant aussi bien le contexte de gauche et celui de droite. C'est aussi le cas de BiLSTM mais le contexte exploité par ce dernier est plus restreint. Aussi il est à noter que Bert a un vocabulaire fixe constitué de \"wordpiece\" (tokens, sous-tokens, caractères), ce qui permet de traiter des problèmes d'out of vocabulary (OOV) de manière plus efficace.\n",
    "\n",
    "## 8 Pourquoi WordPiece ?\n",
    "\n",
    "Prenons `anticonstitutionnellement`. Si l'on entraîne un vecteur pour ce mot il y a peu de chances qu'on en trouve beaucoup d'occurrences. Ce mot sera par la suite sous-représenté. En revanche si l'on le décompose en \"anti\", \"constitution\" et \"ment\". Ces trois subtokens seraient plus fréquents (donc plus de samples) dans un grand corpus et la recomposition de ces 3 subtokens constitueraient une meilleure représentation.\n",
    "\n",
    "## 9 Comment le fine-tuning marche concrètement ?\n",
    "\n",
    "p13\n",
    "\n",
    "## 10 Bert sait tout faire ?\n",
    "\n",
    "p16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table des matières",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
