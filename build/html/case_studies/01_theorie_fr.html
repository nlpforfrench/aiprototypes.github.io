<!doctype html>
<html class="no-js">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />
<link rel="index" title="Index" href="../genindex.html" /><link rel="search" title="Search" href="../search.html" />

    <meta name="generator" content="sphinx-3.5.1, furo 2021.02.28.beta28"/>
        <title>10 questions sur Bert 🇫🇷 - NLP for French</title>
      <link rel="stylesheet" href="../static/styles/furo.css?digest=be5985a4059b5c2cd56ed0804790452beca62674">
    <link rel="stylesheet" href="../static/pygments.css">
    <link media="(prefers-color-scheme: dark)" rel="stylesheet" href="../static/pygments_dark.css">
    


<style>
  :root {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  @media (prefers-color-scheme: dark) {
    :root {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
  }

  /* For allowing end-user-specific overrides */
  .override-light {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  .override-dark {
    --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
  }
</style><link rel="stylesheet" type="text/css" href="../static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../static/gallery.css" />
    <link rel="stylesheet" type="text/css" href="../static/custom.css" />
    <link rel="stylesheet" href="../static/styles/furo-extensions.css?digest=d391b54134226e4196576da3bdb6dddb7e05ba2b"></head>
  <body dir="">
    
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none"
      stroke-width="1.5" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round">
      <path stroke="none" d="M0 0h24v24H0z"/>
      <line x1="4" y1="6" x2="20" y2="6" />
      <line x1="10" y1="12" x2="20" y2="12" />
      <line x1="6" y1="18" x2="20" y2="18" />
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none"
      stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"
      class="feather feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none"
      stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"
      class="feather feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation"></label>
<label class="overlay toc-overlay" for="__toc"></label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">NLP for French</div></a>
    </div>
    <div class="header-right">
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  
  <div class="sidebar-logo-container">
    <img class="sidebar-logo only-light" src="../static/nlp_Logo.png" alt="Light Logo"/>
    <img class="sidebar-logo only-dark" src="../static/nlp_Logo.png" alt="Dark Logo"/>
  </div>
  
  <span class="sidebar-brand-text">NLP for French</span>
  
</a><form class="sidebar-search-container" method="get" action="../search.html">
  <input class="sidebar-search" placeholder=Search name="q">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form><div class="sidebar-scroll"><div class="sidebar-tree">
  <ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../nlp/index.html">NLP and Machine Learning-related</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label for="toctree-checkbox-1"><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../nlp/01_compara_anno_fr.html">Comparer Spacy, StanfordNLP et TreeTagger sur un corpus oral et un corpus de presse 🇫🇷</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp/02_classification_prenoms_fr.html">Classification de prénoms en genre (masculin/féminin) 🇫🇷</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp/03_classification_lemonde_fr.html">Text Classification: du TF-IDF aux word embeddings en passant par features expertes 🇫🇷</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../transformers/index.html">Transformers in NLP with PyTorch, TensorFlow and Hugging Face</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label for="toctree-checkbox-2"><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../transformers/01_theorie_en.html">10 questions on Bert 🇬🇧</a></li>
<li class="toctree-l2"><a class="reference internal" href="../transformers/01_theorie_fr.html">10 questions sur Bert 🇫🇷</a></li>
<li class="toctree-l2"><a class="reference internal" href="../transformers/02_firstBert_fr.html">Classification de commentaires avec Camembert sans prise de tête : les fondamentaux 🇫🇷</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../better_programmer/index.html">Better Programmer</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label for="toctree-checkbox-3"><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../better_programmer/01_python_fr.html">Mieux programmer en Python 🇫🇷</a></li>
<li class="toctree-l2"><a class="reference internal" href="../better_programmer/02_git3_en.html">A serious guide to git 🇬🇧</a></li>
<li class="toctree-l2"><a class="reference internal" href="../better_programmer/03_jupyter_remote_pycharm.html">Connect to remote jupyter notebook in Pycharm</a></li>
<li class="toctree-l2"><a class="reference internal" href="../better_programmer/04_oop_web_scraping_en.html">Understand objected-oriented programming (OOP) by building a minimal Web Scraping framework 🇬🇧</a></li>
<li class="toctree-l2"><a class="reference internal" href="../better_programmer/05_oop_web_scraping_cooper_en.html">Be a responsible programmer when doing Object-Oriented Programming 🇬🇧</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../algo/index.html">Algorithms and data structures by examples in Python</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label for="toctree-checkbox-4"><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../algo/01_intro_en.html">Algorithm or many ways of solving a problem 🇬🇧</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algo/01_intro_fr.html">Algorithme ou plusieurs façons de résoudre un problème 🇫🇷</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algo/02_ds_en.html">Data structures or many ways of organizing your stuff 🇬🇧</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algo/099algo_map.html">Roadmap and cheatsheet of algorithms and data structures</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../web/index.html">Web Related</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label for="toctree-checkbox-5"><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../web/01_lemonde_en.html">Complete tutorial on scraping French news from Le Monde 🇬🇧</a></li>
<li class="toctree-l2"><a class="reference internal" href="../web/01_lemonde_fr.html">Scraper « le monde » et construire ton propre corpus 🇫🇷</a></li>
<li class="toctree-l2"><a class="reference internal" href="../web/02_forum_en.html">On your way to scraping French forums 🇬🇧</a></li>
<li class="toctree-l2"><a class="reference internal" href="../web/03_django_en.html">Deploying Django app on Ubuntu at digitalocean + SSL certificate 🇬🇧</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../linguistique_informatique/index.html">Computational Linguistics in R</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label for="toctree-checkbox-6"><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../linguistique_informatique/01_zipf_fr.html">La loi de Zipf illustrée avec gutenbergr en R 🇫🇷</a></li>
<li class="toctree-l2"><a class="reference internal" href="../linguistique_informatique/02_mca_ergatif_fr.html">Analyse des Correspondances Multiples : le cas de l’ergatif en warlipiri 🇫🇷</a></li>
<li class="toctree-l2"><a class="reference internal" href="../linguistique_informatique/03_pca_inclusion_fr.html">Analyse en composantes principales (PCA) : prépositions d’inclusion en français 🇫🇷</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../high_performance_python/index.html">High performance computing</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label for="toctree-checkbox-7"><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../high_performance_python/01_parallel_primer.html">Parallelization in Python: a beginner’s guide (1, using map)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../codebase/index.html">My Codebase</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label for="toctree-checkbox-8"><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../codebase/01_bash.html">My codebase for bash/shell script (macOS)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../codebase/02_tmux.html">Tumux-related code</a></li>
<li class="toctree-l2"><a class="reference internal" href="../codebase/03_python.html">My python codebase (including packages/libraries)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../codebase/05_pytorch.html">Pytorch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../codebase/06_pandas.html">Pandas codebase</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../math/index.html">Mathematics in Machine Learning and NLP</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label for="toctree-checkbox-9"><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../math/01_math_fr.html">Machine Learning : algorithmes et mathématiques 🇫🇷</a></li>
<li class="toctree-l2"><a class="reference internal" href="../math/01_math_fr.html">Machine Learning : algorithmes et mathématiques 🇫🇷</a></li>
</ul>
</li>
</ul>

</div>
</div>
      </div>
      
    </div>
  </aside>
  <main class="main">
    <div class="content">
      <article role="main">
        <label class="toc-overlay-icon toc-content-icon" for="__toc">
          <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
        </label>
        <section id="questions-sur-bert">
<h1>10 questions sur Bert 🇫🇷<a class="headerlink" href="#questions-sur-bert" title="Permalink to this headline">¶</a></h1>
<p><a class="reference external" href="https://scholar.google.fr/citations?user=vKAMMpwAAAAJ&amp;hl=en">Xiaooou Wang</a></p>
<section id="qu-est-ce-bert">
<h2>Qu’est-ce Bert ?<a class="headerlink" href="#qu-est-ce-bert" title="Permalink to this headline">¶</a></h2>
<p>La composante principale de Bert est un réseau de neurones de 12 couches qui traite du texte. La couche finale de Bert peut être adapté à ta guise pourvu qu’elle soit pertinente à tes tâches (reconnaissance des entités nommées, analyse sentimentale…).</p>
</section>
<section id="qu-est-ce-roberta-xlnet-et-albert">
<h2>Qu’est-ce RoBERTa, XLNet, et ALBERT ?<a class="headerlink" href="#qu-est-ce-roberta-xlnet-et-albert" title="Permalink to this headline">¶</a></h2>
<p>Des modèles dérivés de Bert qui sont parvenus à de meilleures performances sur des benchmarks que Bert.</p>
</section>
<section id="ai-je-besoin-de-comprendre-la-recurrence-des-modeles-cnn-ou-rnn-pour-saisir-les-subtilites-de-bert">
<h2>Ai-je besoin de comprendre la récurrence (des modèles CNN ou RNN) pour saisir les subtilités de Bert ?<a class="headerlink" href="#ai-je-besoin-de-comprendre-la-recurrence-des-modeles-cnn-ou-rnn-pour-saisir-les-subtilites-de-bert" title="Permalink to this headline">¶</a></h2>
<p>Non, Bert est basé sur l’architecture transformer. En plus il n’y a que la partie encoder qui est concernée.</p>
</section>
<section id="c-est-quoi-alors-la-difference-entre-rnn-et-transformer">
<h2>C’est quoi alors la différence entre RNN et Transformer ?<a class="headerlink" href="#c-est-quoi-alors-la-difference-entre-rnn-et-transformer" title="Permalink to this headline">¶</a></h2>
<p>Les réseaux de neurones récurrents ne “regardent” qu’un seul mot à la fois et utilisent des hidden states pour stocker, filtrer et intégrer les informations du passé. Cela rend le modèle très compliqué rien qu’au niveau de la backpropagation (rétropropagation du gradient).</p>
<p>Attention, les RNNs constituent un champ de recherches à part entière. Cela mérite une autre série de tutoriels.</p>
</section>
<section id="lien-entre-pre-training-fine-tuning-et-apprentissage-de-transfert">
<h2>Lien entre Pre-training, Fine-Tuning et apprentissage de transfert ?<a class="headerlink" href="#lien-entre-pre-training-fine-tuning-et-apprentissage-de-transfert" title="Permalink to this headline">¶</a></h2>
<p>Bert est pré-entraîné sur un énorme corpus avec deux tâches. Vu la spécificité de ces deux tâches il est supposé (attention c’est juste une hypothèse) que le modèle acquerra une compréhension profondément contextualisée des mots (concrètement ce sont des embeddings). Mais ce modèle out-of-box n’est pas apte à des tâches en aval (downstream tasks) qui demandent un réapprentissage sur des corpus spécifiques (fine-tuning). L’étude de ceux deux processus relève de l’apprentissage de transfert.</p>
</section>
<section id="quels-interets">
<h2>Quels intérêts ?<a class="headerlink" href="#quels-interets" title="Permalink to this headline">¶</a></h2>
<p>Entraîner un modèle pour une tâche spécifique demande souvent un énorme corpus d’un domaine particulier qui n’est pas forcément disponible. En plus c’est très couteux en termes de temps et de calcul. Le processus de fine-tuning, en plus d’économiser du temps, arrive à de meilleures performances qu’un modèle entraîné à partir de zéro et fait maison.</p>
<p>Attention ! Cela ne signifie pas que le fine-tuning prend quelques secondes, il est juste rapide par rapport au pré-entrainement de Bert et aux modèles faits maison. Si l’on remonte un peu dans le temps, les bénéfices de cette approche ont été d’abord constatés en reconnaissance d’images (branche de computer vision <code class="docutils literal notranslate"><span class="pre">la</span> <span class="pre">traduction</span> <span class="pre">en</span> <span class="pre">français,</span> <span class="pre">vision</span> <span class="pre">par</span> <span class="pre">ordinateur,</span> <span class="pre">est</span> <span class="pre">maladroite</span> <span class="pre">à</span> <span class="pre">mon</span> <span class="pre">avis</span></code>). Théoriquement cela a peu de choses de nouveau.</p>
</section>
<section id="sur-quelles-taches-est-entraine-bert">
<h2>Sur quelles tâches est entraîné Bert ?<a class="headerlink" href="#sur-quelles-taches-est-entraine-bert" title="Permalink to this headline">¶</a></h2>
<p><img alt="" src="../_images/0b53924e1.png"/></p>
<p>Tout comme pour word embedding de Mikolov, ce sont des fake tasks ou tâches de pré-entraînement. La première (Masked Language Model) consiste à deviner un mot masqué dans une phrase (comme le mot cool) et la deuxième (Next Sentence Prediction) à déterminer si la deuxième d’une paire de phrases (comme la phrase en orange) constitue la suite de la première.</p>
</section>
<section id="quels-avantages-des-embeddings-bert">
<h2>Quels avantages des embeddings Bert ?<a class="headerlink" href="#quels-avantages-des-embeddings-bert" title="Permalink to this headline">¶</a></h2>
<p>Comme le mot est masqué à l’intérieur de la phrase, Bert est censé acquérir une représentation contextualisée utilisant aussi bien le contexte de gauche et celui de droite. C’est aussi le cas de BiLSTM mais le contexte exploité par ce dernier est plus restreint. Aussi il est à noter que Bert a un vocabulaire fixe constitué de “wordpiece” (tokens, sous-tokens, caractères), ce qui permet de traiter des problèmes d’out of vocabulary (OOV) de manière plus efficace.</p>
</section>
<section id="pourquoi-wordpiece">
<h2>Pourquoi WordPiece ?<a class="headerlink" href="#pourquoi-wordpiece" title="Permalink to this headline">¶</a></h2>
<p>Prenons <code class="docutils literal notranslate"><span class="pre">anticonstitutionnellement</span></code>. Si l’on entraîne un vecteur pour ce mot il y a peu de chances qu’on en trouve beaucoup d’occurrences. Ce mot sera par la suite sous-représenté. En revanche si l’on le décompose en “anti”, “constitution” et “ment”. Ces trois subtokens seraient plus fréquents (donc plus de samples) dans un grand corpus et la recomposition de ces 3 subtokens constitueraient une meilleure représentation.</p>
</section>
<section id="comment-le-fine-tuning-marche-concretement">
<h2>Comment le fine-tuning marche concrètement ?<a class="headerlink" href="#comment-le-fine-tuning-marche-concretement" title="Permalink to this headline">¶</a></h2>
<p>Cette question est liée à la question. Le processus de fine-tuning dépend de la dernière couche que vous rajoutez à Bert. Prenons l’analyse sentimentale comme un exemple. La figure créée par nos soins illustre ce principe fort simple. A la suite de 12 couches Bert on rajoute un classifieur qui, en fonction du embedding du token CLS (qui englobe l’info de toute la phrase), prédit s’il s’agit d’un commentaire positif ou négatif.</p>
<p><img alt="" src="../_images/3ed814141.png"/></p>
<div class="note admonition">
<p class="admonition-title">Next ?</p>
<p>Des tutoriels sur</p>
<ul class="simple">
<li><p>NER (reconnaissances des entités nommées)</p></li>
<li><p>classification de textes</p></li>
<li><p>fine-tuning sur corpus spécifique</p></li>
</ul>
<p>Stay tuned!</p>
</div>
</section>
</section>

      </article>
      <footer>
        
        <div class="related-pages">
          
          
        </div>

        <div class="related-information">
              Copyright &#169; 2022, Xiaoou Wang
            |
            Built with <a href="https://www.sphinx-doc.org/">Sphinx</a>
              and
              <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
              <a href="https://github.com/pradyunsg/furo">Furo theme</a>.
            |
            <a class="muted-link" href="../sources/case_studies/01_theorie_fr.md.txt"
               rel="nofollow">
              Show Source
            </a>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            Contents
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">10 questions sur Bert 🇫🇷</a><ul>
<li><a class="reference internal" href="#qu-est-ce-bert">Qu’est-ce Bert ?</a></li>
<li><a class="reference internal" href="#qu-est-ce-roberta-xlnet-et-albert">Qu’est-ce RoBERTa, XLNet, et ALBERT ?</a></li>
<li><a class="reference internal" href="#ai-je-besoin-de-comprendre-la-recurrence-des-modeles-cnn-ou-rnn-pour-saisir-les-subtilites-de-bert">Ai-je besoin de comprendre la récurrence (des modèles CNN ou RNN) pour saisir les subtilités de Bert ?</a></li>
<li><a class="reference internal" href="#c-est-quoi-alors-la-difference-entre-rnn-et-transformer">C’est quoi alors la différence entre RNN et Transformer ?</a></li>
<li><a class="reference internal" href="#lien-entre-pre-training-fine-tuning-et-apprentissage-de-transfert">Lien entre Pre-training, Fine-Tuning et apprentissage de transfert ?</a></li>
<li><a class="reference internal" href="#quels-interets">Quels intérêts ?</a></li>
<li><a class="reference internal" href="#sur-quelles-taches-est-entraine-bert">Sur quelles tâches est entraîné Bert ?</a></li>
<li><a class="reference internal" href="#quels-avantages-des-embeddings-bert">Quels avantages des embeddings Bert ?</a></li>
<li><a class="reference internal" href="#pourquoi-wordpiece">Pourquoi WordPiece ?</a></li>
<li><a class="reference internal" href="#comment-le-fine-tuning-marche-concretement">Comment le fine-tuning marche concrètement ?</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </main>
</div>
    <script id="documentation_options" data-url_root="../" src="../static/documentation_options.js"></script>
    <script src="../static/jquery.js"></script>
    <script src="../static/underscore.js"></script>
    <script src="../static/doctools.js"></script>
    <script src="../static/clipboard.min.js"></script>
    <script src="../static/copybutton.js"></script>
    <script src="../static/tabs.js"></script>
    <script >
</script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" integrity="sha512-iBBXm8fW90+nuLcSKlbmrPcLa0OT92xO1BIsZ+ywDWZCvqsWgccV3gFoRBv0z+8dLJgyAHIhR35VZc2oM/gI1w==" crossorigin="anonymous" />
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-58WGY2PHYB"></script>
<script src='https://kit.fontawesome.com/a076d05399.js' crossorigin='anonymous'></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-58WGY2PHYB');
</script>
<!-- Default Statcounter code for nlpinfrench
http://nlpinfrench.fr -->
<script type="text/javascript">
var sc_project=12500373;
var sc_invisible=1;
var sc_security="def60251";
</script>
<script type="text/javascript"
src="https://www.statcounter.com/counter/counter.js"
async></script>
<noscript><div class="statcounter"><a title="Web Analytics
Made Easy - StatCounter" href="https://statcounter.com/"
target="_blank"><img class="statcounter"
src="https://c.statcounter.com/12500373/0/def60251/1/"
alt="Web Analytics Made Easy -
StatCounter"></a></div></noscript>
<!-- End of Statcounter Code -->
<script src="https://cdn.jsdelivr.net/gh/cferdinandi/gumshoe@4.0/dist/gumshoe.polyfills.min.js"></script>
</script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../static/custom.js"></script>
    <script src="../static/scripts/main.js?digest=e931d09b2a40c1bb82b542effe772014573baf67"></script></body>
</html>