# 10 questions sur Bert üá´üá∑

[Xiaooou Wang](https://https://scholar.google.fr/citations?user=vKAMMpwAAAAJ&hl=en)

## Qu'est-ce Bert ?

La composante principale de Bert est un r√©seau de neurones de 12 couches qui traite du texte. La couche finale de Bert peut √™tre adapt√© √† ta guise pourvu qu'elle soit pertinente √† tes t√¢ches (reconnaissance des entit√©s nomm√©es, analyse sentimentale...).

## Qu'est-ce RoBERTa, XLNet, et ALBERT ?

Des mod√®les d√©riv√©s de Bert qui sont parvenus √† de meilleures performances sur des benchmarks que Bert.

## Ai-je besoin de comprendre la r√©currence (des mod√®les CNN ou RNN) pour saisir les subtilit√©s de Bert ?

Non, Bert est bas√© sur l'architecture transformer. En plus il n'y a que la partie encoder qui est concern√©e.

## C'est quoi alors la diff√©rence entre RNN et Transformer ?

Les r√©seaux de neurones r√©currents ne "regardent" qu'un seul mot √† la fois et utilisent des hidden states pour stocker, filtrer et int√©grer les informations du pass√©. Cela rend le mod√®le tr√®s compliqu√© rien qu'au niveau de la backpropagation (r√©tropropagation du gradient).

Attention, les RNNs constituent un champ de recherches √† part enti√®re. Cela m√©rite une autre s√©rie de tutoriels.

## Lien entre Pre-training, Fine-Tuning et apprentissage de transfert ?

Bert est pr√©-entra√Æn√© sur un √©norme corpus avec deux t√¢ches. Vu la sp√©cificit√© de ces deux t√¢ches il est suppos√© (attention c'est juste une hypoth√®se) que le mod√®le acquerra une compr√©hension profond√©ment contextualis√©e des mots (concr√®tement ce sont des embeddings). Mais ce mod√®le out-of-box n'est pas apte √† des t√¢ches en aval (downstream tasks) qui demandent un r√©apprentissage sur des corpus sp√©cifiques (fine-tuning). L'√©tude de ceux deux processus rel√®ve de l'apprentissage de transfert.

## Quels int√©r√™ts ?

Entra√Æner un mod√®le pour une t√¢che sp√©cifique demande souvent un √©norme corpus d'un domaine particulier qui n'est pas forc√©ment disponible. En plus c'est tr√®s couteux en termes de temps et de calcul. Le processus de fine-tuning, en plus d'√©conomiser du temps, arrive √† de meilleures performances qu'un mod√®le entra√Æn√© √† partir de z√©ro et fait maison.

Attention ! Cela ne signifie pas que le fine-tuning prend quelques secondes, il est juste rapide par rapport au pr√©-entrainement de Bert et aux mod√®les faits maison. Si l'on remonte un peu dans le temps, les b√©n√©fices de cette approche ont √©t√© d'abord constat√©s en reconnaissance d'images (branche de computer vision `la traduction en fran√ßais, vision par ordinateur, est maladroite √† mon avis`). Th√©oriquement cela a peu de choses de nouveau.

## Sur quelles t√¢ches est entra√Æn√© Bert ?

![](01_theorie_en_images/0b53924e.png)

Tout comme pour word embedding de Mikolov, ce sont des fake tasks ou t√¢ches de pr√©-entra√Ænement. La premi√®re (Masked Language Model) consiste √† deviner un mot masqu√© dans une phrase (comme le mot cool) et la deuxi√®me (Next Sentence Prediction) √† d√©terminer si la deuxi√®me d'une paire de phrases (comme la phrase en orange) constitue la suite de la premi√®re.

## Quels avantages des embeddings Bert ?

Comme le mot est masqu√© √† l'int√©rieur de la phrase, Bert est cens√© acqu√©rir une repr√©sentation contextualis√©e utilisant aussi bien le contexte de gauche et celui de droite. C'est aussi le cas de BiLSTM mais le contexte exploit√© par ce dernier est plus restreint. Aussi il est √† noter que Bert a un vocabulaire fixe constitu√© de "wordpiece" (tokens, sous-tokens, caract√®res), ce qui permet de traiter des probl√®mes d'out of vocabulary (OOV) de mani√®re plus efficace.

## Pourquoi WordPiece ?

Prenons `anticonstitutionnellement`. Si l'on entra√Æne un vecteur pour ce mot il y a peu de chances qu'on en trouve beaucoup d'occurrences. Ce mot sera par la suite sous-repr√©sent√©. En revanche si l'on le d√©compose en "anti", "constitution" et "ment". Ces trois subtokens seraient plus fr√©quents (donc plus de samples) dans un grand corpus et la recomposition de ces 3 subtokens constitueraient une meilleure repr√©sentation.

## Comment le fine-tuning marche concr√®tement ?

Cette question est li√©e √† la question. Le processus de fine-tuning d√©pend de la derni√®re couche que vous rajoutez √† Bert. Prenons l'analyse sentimentale comme un exemple. La figure cr√©√©e par nos soins illustre ce principe fort simple. A la suite de 12 couches Bert on rajoute un classifieur qui, en fonction du embedding du token CLS (qui englobe l'info de toute la phrase), pr√©dit s'il s'agit d'un commentaire positif ou n√©gatif.

![](01_theorie_en_images/3ed81414.png)

```{admonition} Next ?
:class: note

Des tutoriels sur

* NER (reconnaissances des entit√©s nomm√©es)
* classification de textes
* fine-tuning sur corpus sp√©cifique

Stay tuned!
```
